# DSA4212 Transformer Project

## Overview
This project implements a transformer architecture for next-character prediction on the text8 dataset. We implement, train, and optimize transformer models while conducting detailed analysis of the results to identify the optimal configuration. The project includes experimentation with various hyperparameters, training strategies, and model interpretations to understand transformer behavior on character-level language modeling tasks.

## Project Structure
```
├── models/            # Model implementations
├── Notebooks/         # Jupyter notebooks for experimentation
├── Report/            # Typst File for Report Writing
├── Loss_Curves/       # Loss curves obtained during training
├── requirements.txt   # Python dependencies
└── README.md          # This file
```

## Installation
```bash
pip install -r requirements.txt
```

## Authors
Andrew Lyem, Min Aung Oo, Su Jia Ying, Joanne