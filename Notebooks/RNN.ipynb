{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3239162f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import time\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93721bad",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea216b2a",
   "metadata": {},
   "source": [
    "## Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c2cfe072",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 1\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 1\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 1\n",
      "    })\n",
      "})\n",
      "90000000\n",
      " anarchism originated as a term of abuse first used against early working class radicals including the diggers of the english revolution and the sans culottes of the french revolution whilst the term is still used in a pejorative way to describe any act that used violent means to destroy the organization of society it has also been taken up as a positive label by self defined anarchists the word anarchism is derived from the greek without archons ruler chief king anarchism as a political philoso\n"
     ]
    }
   ],
   "source": [
    "# Load the text8 dataset\n",
    "dataset = load_dataset(\"afmck/text8\")\n",
    "\n",
    "# The dataset has one long text string\n",
    "print(dataset)\n",
    "\n",
    "# Check datasize\n",
    "print(len(dataset[\"train\"][0][\"text\"]))\n",
    "\n",
    "# Preview first 500 characters\n",
    "print(dataset[\"train\"][0][\"text\"][:500])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9301a2e3",
   "metadata": {},
   "source": [
    "## Splice Small Subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "df3a3f7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of small text subset: 500000 characters\n",
      "Length of training text: 450000 characters\n",
      "Length of validation text: 50000 characters\n"
     ]
    }
   ],
   "source": [
    "full_text = dataset[\"train\"][0][\"text\"]\n",
    "# extract small subset for faster experiments\n",
    "small_text = dataset[\"train\"][0][\"text\"][:500000]  # first 500,000 chars\n",
    "print(f\"Length of small text subset: {len(small_text)} characters\")\n",
    "# split into training and validation sets\n",
    "train_text = small_text[:450000]  # first 450,000 chars for training\n",
    "val_text = small_text[450000:]    # last 50,000 chars for validation\n",
    "print(f\"Length of training text: {len(train_text)} characters\")\n",
    "print(f\"Length of validation text: {len(val_text)} characters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "910d81cb",
   "metadata": {},
   "source": [
    "# Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a97d72d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test string: hello world\n"
     ]
    }
   ],
   "source": [
    "# Build vocabulary (lowercase + space + a few punctuations)\n",
    "char_set = list(\"abcdefghijklmnopqrstuvwxyz \")\n",
    "char_to_int = {ch:i for i,ch in enumerate(char_set)}\n",
    "int_to_char = {i:ch for ch,i in char_to_int.items()}\n",
    "\n",
    "def encode(s):\n",
    "    \"\"\"Encode string to array of integers\"\"\"\n",
    "    ids = [char_to_int[c] for c in s]\n",
    "    return np.array(ids, dtype=np.uint8)  # use np.uint8 to save space\n",
    "\n",
    "def decode(ids):\n",
    "    \"\"\"Decode array of integers to string\"\"\"\n",
    "    return ''.join(int_to_char[i] for i in ids)\n",
    "# Test encoding and decoding\n",
    "test_str = \"hello world\"\n",
    "encoded = encode(test_str)\n",
    "decoded = decode(encoded)\n",
    "assert test_str == decoded, \"Encoding/decoding failed\"\n",
    "print(f\"Test string: {test_str}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "dd1ae114",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tokenize the Text Data\n",
    "train_text_int = encode(train_text)\n",
    "test_text_int = encode(val_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f787197",
   "metadata": {},
   "source": [
    "# Data Loader"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c88a6b9d",
   "metadata": {},
   "source": [
    "## Wrap in pyTorch Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e5ca98b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import numpy as np\n",
    "\n",
    "class Text8Dataset(Dataset):\n",
    "    def __init__(self, data, seq_len):\n",
    "        \"\"\"\n",
    "        data: 1D numpy array of token IDs (uint8 or int)\n",
    "        seq_len: length T of each input sequence\n",
    "        \"\"\"\n",
    "        self.data = data\n",
    "        self.seq_len = seq_len\n",
    "\n",
    "    def __len__(self):\n",
    "        # Number of possible sequences\n",
    "        return len(self.data) - self.seq_len\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # input is [idx : idx+T]\n",
    "        # target is next char [idx+1 : idx+T+1]\n",
    "        x = self.data[idx : idx + self.seq_len]\n",
    "        y = self.data[idx + 1 : idx + 1 + self.seq_len]\n",
    "\n",
    "        # Convert to torch tensors\n",
    "        x = torch.tensor(x, dtype=torch.long)\n",
    "        y = torch.tensor(y, dtype=torch.long)\n",
    "        return x, y\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eb3e900",
   "metadata": {},
   "source": [
    "## Create DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "845a8605",
   "metadata": {},
   "outputs": [],
   "source": [
    "SEQ_LEN = 64\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "train_dataset = Text8Dataset(train_text_int, seq_len=SEQ_LEN)\n",
    "test_dataset  = Text8Dataset(test_text_int,  seq_len=SEQ_LEN)\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=True,\n",
    "    drop_last=True,\n",
    ")\n",
    "\n",
    "test_loader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    shuffle=False,\n",
    "    drop_last=True,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6e757f45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input batch shape: torch.Size([128, 64])\n",
      "Target batch shape: torch.Size([128, 64])\n",
      "Example decoded input:  and secretary of state william seward without his bodyguard war\n",
      "Example decoded target: and secretary of state william seward without his bodyguard ward\n"
     ]
    }
   ],
   "source": [
    "# Sanity check the DataLoader\n",
    "# Should be shifted by one between input and target\n",
    "x, y = next(iter(train_loader))\n",
    "print(\"Input batch shape:\", x.shape)   # (B, T)\n",
    "print(\"Target batch shape:\", y.shape)  # (B, T)\n",
    "print(\"Example decoded input:\", decode(x[0].numpy()))\n",
    "print(\"Example decoded target:\", decode(y[0].numpy()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb94fa2f",
   "metadata": {},
   "source": [
    "# LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6fd14a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class LSTMCharModel(nn.Module):\n",
    "    def __init__(self, vocab_size=27, embed_dim=128, hidden_dim=256, num_layers=2):\n",
    "        super().__init__()\n",
    "\n",
    "        # 1. Embedding layer\n",
    "        self.embed = nn.Embedding(vocab_size, embed_dim)\n",
    "\n",
    "        # 2. LSTM\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=embed_dim,\n",
    "            hidden_size=hidden_dim,\n",
    "            num_layers=num_layers,\n",
    "            batch_first=True\n",
    "        )\n",
    "\n",
    "        # 3. Output projection\n",
    "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
    "\n",
    "    def forward(self, x, hidden=None):\n",
    "        \"\"\"\n",
    "        x: (B, T)\n",
    "        returns logits: (B, T, vocab_size)\n",
    "        \"\"\"\n",
    "        x = self.embed(x)              # (B, T, embed_dim)\n",
    "        out, hidden = self.lstm(x, hidden)  # (B, T, hidden_dim)\n",
    "        logits = self.fc(out)          # (B, T, vocab_size)\n",
    "        return logits, hidden\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c9614865",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logits shape: torch.Size([128, 64, 27])\n"
     ]
    }
   ],
   "source": [
    "# Sanity check the model\n",
    "model = LSTMCharModel(vocab_size=27)\n",
    "x, y = next(iter(train_loader))\n",
    "logits, _ = model(x)\n",
    "\n",
    "print(\"Logits shape:\", logits.shape)\n",
    "# Should be (B, T, vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28959b59",
   "metadata": {},
   "source": [
    "# Loss function and optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3a014b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "# Pytorch's CrossEntropyLoss expects inputs of shape (B*T, C) and targets of shape (B*T)\n",
    "logits_flat = logits.reshape(-1, 27)\n",
    "targets_flat = y.reshape(-1)\n",
    "loss = criterion(logits_flat, targets_flat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1b0c3dca",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f8be827",
   "metadata": {},
   "source": [
    "## Accuracy Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "125cd1b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_accuracy(logits, targets):\n",
    "    \"\"\"\n",
    "    logits: (B, T, vocab)\n",
    "    targets: (B, T)\n",
    "    Returns:\n",
    "        acc_all: accuracy over all positions\n",
    "        acc_last: accuracy at last position only\n",
    "    \"\"\"\n",
    "    preds = logits.argmax(dim=-1)          # (B, T)\n",
    "\n",
    "    correct_all = (preds == targets).float().mean().item()\n",
    "    correct_last = (preds[:, -1] == targets[:, -1]).float().mean().item()\n",
    "\n",
    "    return correct_all, correct_last\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cff6251",
   "metadata": {},
   "source": [
    "# Training Loop"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c1783bf",
   "metadata": {},
   "source": [
    "## Initialization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c35e16ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(\"Using device:\", device)\n",
    "\n",
    "model = LSTMCharModel(vocab_size=27).to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2c2b205b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_lstm(\n",
    "    model,\n",
    "    train_loader,\n",
    "    test_loader,\n",
    "    optimizer,\n",
    "    criterion,\n",
    "    num_epochs=3,\n",
    "):\n",
    "    model.train()\n",
    "\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        total_loss = 0.0\n",
    "\n",
    "        for x, y in train_loader:\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            logits, _ = model(x)  # (B, T, vocab)\n",
    "            loss = criterion(logits.reshape(-1, 27), y.reshape(-1))\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "\n",
    "        # Evaluate\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_loss = 0.0\n",
    "            acc_all_list = []\n",
    "            acc_last_list = []\n",
    "\n",
    "            for x_val, y_val in test_loader:\n",
    "                x_val = x_val.to(device)\n",
    "                y_val = y_val.to(device)\n",
    "\n",
    "                logits, _ = model(x_val)\n",
    "                loss = criterion(logits.reshape(-1, 27), y_val.reshape(-1))\n",
    "                val_loss += loss.item()\n",
    "\n",
    "                acc_all, acc_last = compute_accuracy(logits, y_val)\n",
    "                acc_all_list.append(acc_all)\n",
    "                acc_last_list.append(acc_last)\n",
    "\n",
    "        val_loss /= len(test_loader)\n",
    "        avg_acc_all = sum(acc_all_list) / len(acc_all_list)\n",
    "        avg_acc_last = sum(acc_last_list) / len(acc_last_list)\n",
    "\n",
    "        print(f\"Epoch {epoch}\")\n",
    "        print(f\"  Train Loss: {avg_loss:.4f}\")\n",
    "        print(f\"  Val Loss  : {val_loss:.4f}\")\n",
    "        print(f\"  Accuracy (all positions): {avg_acc_all*100:.2f}%\")\n",
    "        print(f\"  Accuracy (next char)    : {avg_acc_last*100:.2f}%\")\n",
    "        print()\n",
    "\n",
    "        model.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c7dc351c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1\n",
      "  Train Loss: 1.1411\n",
      "  Val Loss  : 1.6842\n",
      "  Accuracy (all positions): 55.58%\n",
      "  Accuracy (next char)    : 56.75%\n",
      "\n",
      "Epoch 2\n",
      "  Train Loss: 0.6355\n",
      "  Val Loss  : 2.3050\n",
      "  Accuracy (all positions): 53.64%\n",
      "  Accuracy (next char)    : 54.55%\n",
      "\n",
      "Epoch 3\n",
      "  Train Loss: 0.4636\n",
      "  Val Loss  : 2.6788\n",
      "  Accuracy (all positions): 53.24%\n",
      "  Accuracy (next char)    : 54.07%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Sanity Training\n",
    "train_lstm(\n",
    "    model=model,\n",
    "    train_loader=train_loader,\n",
    "    test_loader=test_loader,\n",
    "    optimizer=optimizer,\n",
    "    criterion=criterion,\n",
    "    num_epochs=3,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "739aacb2",
   "metadata": {},
   "source": [
    "# Custom Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "50209641",
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_lstm_experiment(\n",
    "    lr=1e-3,\n",
    "    train_batch_size=128,\n",
    "    test_batch_size=512,\n",
    "    seq_len=64,\n",
    "    num_epochs=3,\n",
    "    embed_dim=128,\n",
    "    hidden_dim=256,\n",
    "    num_layers=2,\n",
    "):\n",
    "    \"\"\"\n",
    "    Train an LSTM on text8 with configurable hyperparameters.\n",
    "\n",
    "    Returns a dict with final train/val loss and accuracies.\n",
    "    \"\"\"\n",
    "\n",
    "    # 1. Build datasets and dataloaders for this experiment\n",
    "    train_dataset = Text8Dataset(train_text_int, seq_len=seq_len)\n",
    "    test_dataset  = Text8Dataset(test_text_int,  seq_len=seq_len)\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=train_batch_size,\n",
    "        shuffle=True,\n",
    "        drop_last=True,\n",
    "    )\n",
    "\n",
    "    test_loader = DataLoader(\n",
    "        test_dataset,\n",
    "        batch_size=test_batch_size,\n",
    "        shuffle=False,\n",
    "        drop_last=True,\n",
    "    )\n",
    "\n",
    "    # 2. Build model, loss, and optimizer\n",
    "    vocab_size = 27\n",
    "    model = LSTMCharModel(\n",
    "        vocab_size=vocab_size,\n",
    "        embed_dim=embed_dim,\n",
    "        hidden_dim=hidden_dim,\n",
    "        num_layers=num_layers,\n",
    "    ).to(device)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    print(f\"Starting LSTM experiment:\")\n",
    "    print(f\"  lr={lr}, epochs={num_epochs}, seq_len={seq_len}\")\n",
    "    print(f\"  train_batch_size={train_batch_size}, test_batch_size={test_batch_size}\")\n",
    "    print(f\"  embed_dim={embed_dim}, hidden_dim={hidden_dim}, num_layers={num_layers}\")\n",
    "    print()\n",
    "\n",
    "    # 3. Training loop\n",
    "    for epoch in range(1, num_epochs + 1):\n",
    "        model.train()\n",
    "        total_train_loss = 0.0\n",
    "\n",
    "        for x, y in tqdm(train_loader, desc=f\"Epoch {epoch} [train]\", leave=False):\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            logits, _ = model(x)  # (B, T, vocab)\n",
    "            loss = criterion(logits.reshape(-1, vocab_size), y.reshape(-1))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_train_loss += loss.item()\n",
    "\n",
    "        avg_train_loss = total_train_loss / len(train_loader)\n",
    "\n",
    "        # 4. Evaluation on test set\n",
    "        model.eval()\n",
    "        total_val_loss = 0.0\n",
    "        acc_all_list = []\n",
    "        acc_last_list = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for x_val, y_val in tqdm(test_loader, desc=f\"Epoch {epoch} [val]\", leave=False):\n",
    "                x_val = x_val.to(device)\n",
    "                y_val = y_val.to(device)\n",
    "\n",
    "                logits, _ = model(x_val)\n",
    "                loss = criterion(logits.reshape(-1, vocab_size), y_val.reshape(-1))\n",
    "                total_val_loss += loss.item()\n",
    "\n",
    "                acc_all, acc_last = compute_accuracy(logits, y_val)\n",
    "                acc_all_list.append(acc_all)\n",
    "                acc_last_list.append(acc_last)\n",
    "\n",
    "        avg_val_loss = total_val_loss / len(test_loader)\n",
    "        avg_acc_all = sum(acc_all_list) / len(acc_all_list)\n",
    "        avg_acc_last = sum(acc_last_list) / len(acc_last_list)\n",
    "\n",
    "        print(f\"Epoch {epoch}/{num_epochs}\")\n",
    "        print(f\"  Train loss: {avg_train_loss:.4f}\")\n",
    "        print(f\"  Val loss  : {avg_val_loss:.4f}\")\n",
    "        print(f\"  Accuracy (all positions): {avg_acc_all*100:.2f}%\")\n",
    "        print(f\"  Accuracy (next char)    : {avg_acc_last*100:.2f}%\")\n",
    "        print()\n",
    "\n",
    "    # 5. Return metrics for logging / comparison\n",
    "    return {\n",
    "        \"lr\": lr,\n",
    "        \"train_batch_size\": train_batch_size,\n",
    "        \"test_batch_size\": test_batch_size,\n",
    "        \"seq_len\": seq_len,\n",
    "        \"num_epochs\": num_epochs,\n",
    "        \"embed_dim\": embed_dim,\n",
    "        \"hidden_dim\": hidden_dim,\n",
    "        \"num_layers\": num_layers,\n",
    "        \"final_train_loss\": avg_train_loss,\n",
    "        \"final_val_loss\": avg_val_loss,\n",
    "        \"final_acc_all\": avg_acc_all,\n",
    "        \"final_acc_last\": avg_acc_last,\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04d0ce5b",
   "metadata": {},
   "source": [
    "# Hyper Parameter Experimentation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08f187ee",
   "metadata": {},
   "source": [
    "## LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "986308c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Running lr=0.001 ===\n",
      "Starting LSTM experiment:\n",
      "  lr=0.001, epochs=1, seq_len=64\n",
      "  train_batch_size=128, test_batch_size=512\n",
      "  embed_dim=128, hidden_dim=256, num_layers=2\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "  Train loss: 1.1876\n",
      "  Val loss  : 1.5879\n",
      "  Accuracy (all positions): 56.35%\n",
      "  Accuracy (next char)    : 57.59%\n",
      "\n",
      "Final val loss: 1.5879104960825026\n",
      "Final overall acc: 56.35372830420425 %\n",
      "Final next-char acc: 57.59101159793815 %\n",
      "----------------------------------------\n",
      "=== Running lr=0.0003 ===\n",
      "Starting LSTM experiment:\n",
      "  lr=0.0003, epochs=1, seq_len=64\n",
      "  train_batch_size=128, test_batch_size=512\n",
      "  embed_dim=128, hidden_dim=256, num_layers=2\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "  Train loss: 1.4711\n",
      "  Val loss  : 1.4215\n",
      "  Accuracy (all positions): 56.79%\n",
      "  Accuracy (next char)    : 58.07%\n",
      "\n",
      "Final val loss: 1.4215377114482761\n",
      "Final overall acc: 56.79384211903995 %\n",
      "Final next-char acc: 58.06821842783505 %\n",
      "----------------------------------------\n",
      "=== Running lr=0.0001 ===\n",
      "Starting LSTM experiment:\n",
      "  lr=0.0001, epochs=1, seq_len=64\n",
      "  train_batch_size=128, test_batch_size=512\n",
      "  embed_dim=128, hidden_dim=256, num_layers=2\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "  Train loss: 1.8004\n",
      "  Val loss  : 1.5480\n",
      "  Accuracy (all positions): 53.27%\n",
      "  Accuracy (next char)    : 54.35%\n",
      "\n",
      "Final val loss: 1.5480079564851583\n",
      "Final overall acc: 53.26648200910115 %\n",
      "Final next-char acc: 54.34519974226804 %\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "lr_results = {}   # store per-LR results\n",
    "\n",
    "for lr in [1e-3, 3e-4, 1e-4]:\n",
    "    print(f\"=== Running lr={lr} ===\")\n",
    "\n",
    "    metrics = run_lstm_experiment(\n",
    "        lr=lr,\n",
    "        train_batch_size=128,\n",
    "        test_batch_size=512,\n",
    "        seq_len=64,\n",
    "        num_epochs=1,  # only 1 epoch for fast sweep\n",
    "    )\n",
    "\n",
    "    lr_results[lr] = metrics   # store metrics for plotting\n",
    "\n",
    "    print(\"Final val loss:\", metrics[\"final_val_loss\"])\n",
    "    print(\"Final overall acc:\", metrics[\"final_acc_all\"] * 100, \"%\")\n",
    "    print(\"Final next-char acc:\", metrics[\"final_acc_last\"] * 100, \"%\")\n",
    "    print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6954eebe",
   "metadata": {},
   "source": [
    "## Batch Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0d15b2fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting LSTM experiment:\n",
      "  lr=0.0003, epochs=1, seq_len=64\n",
      "  train_batch_size=64, test_batch_size=512\n",
      "  embed_dim=128, hidden_dim=256, num_layers=2\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "  Train loss: 1.3418\n",
      "  Val loss  : 1.4597\n",
      "  Accuracy (all positions): 57.25%\n",
      "  Accuracy (next char)    : 58.54%\n",
      "\n",
      "Batch 64 → next-char acc: 58.537371134020624 %\n",
      "----------------------------------------\n",
      "Starting LSTM experiment:\n",
      "  lr=0.0003, epochs=1, seq_len=64\n",
      "  train_batch_size=128, test_batch_size=512\n",
      "  embed_dim=128, hidden_dim=256, num_layers=2\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "  Train loss: 1.4769\n",
      "  Val loss  : 1.4201\n",
      "  Accuracy (all positions): 56.81%\n",
      "  Accuracy (next char)    : 58.08%\n",
      "\n",
      "Batch 128 → next-char acc: 58.084326675257735 %\n",
      "----------------------------------------\n",
      "Starting LSTM experiment:\n",
      "  lr=0.0003, epochs=1, seq_len=64\n",
      "  train_batch_size=256, test_batch_size=512\n",
      "  embed_dim=128, hidden_dim=256, num_layers=2\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "  Train loss: 1.6418\n",
      "  Val loss  : 1.4618\n",
      "  Accuracy (all positions): 55.65%\n",
      "  Accuracy (next char)    : 56.87%\n",
      "\n",
      "Batch 256 → next-char acc: 56.87218105670103 %\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "for batch in [64, 128, 256]:\n",
    "    metrics = run_lstm_experiment(\n",
    "        lr=3e-4,\n",
    "        train_batch_size=batch,\n",
    "        test_batch_size=512,\n",
    "        seq_len=64,\n",
    "        num_epochs=1,\n",
    "    )\n",
    "    print(\"Batch\", batch, \"→ next-char acc:\", metrics[\"final_acc_last\"] * 100, \"%\")\n",
    "    print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c438fde5",
   "metadata": {},
   "source": [
    "## Sequence Length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d3600de2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting LSTM experiment:\n",
      "  lr=0.0003, epochs=1, seq_len=64\n",
      "  train_batch_size=128, test_batch_size=512\n",
      "  embed_dim=128, hidden_dim=256, num_layers=2\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "  Train loss: 1.4669\n",
      "  Val loss  : 1.4269\n",
      "  Accuracy (all positions): 56.73%\n",
      "  Accuracy (next char)    : 58.04%\n",
      "\n",
      "Seq Len 64 → next-char acc: 58.0420425257732 %\n",
      "----------------------------------------\n",
      "Starting LSTM experiment:\n",
      "  lr=0.0003, epochs=1, seq_len=128\n",
      "  train_batch_size=128, test_batch_size=512\n",
      "  embed_dim=128, hidden_dim=256, num_layers=2\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                    \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "  Train loss: 1.4093\n",
      "  Val loss  : 1.4112\n",
      "  Accuracy (all positions): 57.87%\n",
      "  Accuracy (next char)    : 58.57%\n",
      "\n",
      "Seq Len 128 → next-char acc: 58.56556056701031 %\n",
      "----------------------------------------\n",
      "Starting LSTM experiment:\n",
      "  lr=0.0003, epochs=1, seq_len=256\n",
      "  train_batch_size=128, test_batch_size=512\n",
      "  embed_dim=128, hidden_dim=256, num_layers=2\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "  Train loss: 1.3351\n",
      "  Val loss  : 1.4627\n",
      "  Accuracy (all positions): 57.54%\n",
      "  Accuracy (next char)    : 57.92%\n",
      "\n",
      "Seq Len 256 → next-char acc: 57.91921713917526 %\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "for seq in [64, 128, 256]:\n",
    "    metrics = run_lstm_experiment(\n",
    "        lr=3e-4,\n",
    "        train_batch_size=128,\n",
    "        test_batch_size=512,\n",
    "        seq_len=seq,\n",
    "        num_epochs=1,\n",
    "    )\n",
    "    print(\"Seq Len\", seq, \"→ next-char acc:\", metrics[\"final_acc_last\"] * 100, \"%\")\n",
    "    print(\"-\" * 40)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9dec114",
   "metadata": {},
   "source": [
    "## Model Size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09c44c4e",
   "metadata": {},
   "source": [
    "### Medium Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b6c7eda7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting LSTM experiment:\n",
      "  lr=0.0003, epochs=1, seq_len=128\n",
      "  train_batch_size=128, test_batch_size=512\n",
      "  embed_dim=256, hidden_dim=512, num_layers=2\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                    "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "  Train loss: 1.0088\n",
      "  Val loss  : 2.1696\n",
      "  Accuracy (all positions): 53.50%\n",
      "  Accuracy (next char)    : 53.86%\n",
      "\n",
      "Model Size: Medium → next-char acc: 53.86195231958762 %\n",
      "----------------------------------------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "metrics = run_lstm_experiment(\n",
    "    lr=3e-4,\n",
    "    train_batch_size=128,\n",
    "    test_batch_size=512,\n",
    "    seq_len=128,\n",
    "    num_epochs=1,\n",
    "    embed_dim=256,\n",
    "    hidden_dim=512,\n",
    "    num_layers=2,\n",
    "    )\n",
    "print(\"Model Size: Medium → next-char acc:\", metrics[\"final_acc_last\"] * 100, \"%\")\n",
    "print(\"-\" * 40)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp311",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
