{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04b62127",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/exyzt/projects/DSA4212-Transformer-Project/venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import flax.linen as nn\n",
    "from flax.linen import attention as attn\n",
    "from typing import Any, Callable\n",
    "import optax\n",
    "import time\n",
    "import wandb\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "root = Path.cwd().parent\n",
    "sys.path.append(str(root))\n",
    "import models.decoder_only_transformer as decoder_only_transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df524230",
   "metadata": {},
   "source": [
    "### Check GPU Availability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "48330a23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JAX is using: gpu\n",
      "All available devices: [CudaDevice(id=0)]\n"
     ]
    }
   ],
   "source": [
    "print(\"JAX is using:\", jax.default_backend())\n",
    "print(\"All available devices:\", jax.devices())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e08b7121",
   "metadata": {},
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d25faee",
   "metadata": {},
   "source": [
    "#### Loading the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0b9ea7c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 1\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 1\n",
      "    })\n",
      "    test: Dataset({\n",
      "        features: ['text'],\n",
      "        num_rows: 1\n",
      "    })\n",
      "})\n",
      " anarchism originated as a term of abuse first used against early working class radicals including the diggers of the english revolution and the sans culottes of the french revolution whilst the term is still used in a pejorative way to describe any act that used violent means to destroy the organization of society it has also been taken up as a positive label by self defined anarchists the word anarchism is derived from the greek without archons ruler chief king anarchism as a political philoso\n"
     ]
    }
   ],
   "source": [
    "dataset = load_dataset(\"afmck/text8\")\n",
    "\n",
    "print(dataset)\n",
    "print(dataset[\"train\"][0][\"text\"][:500])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e499b966",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of train text: 90,000,000 characters\n",
      "Length of validation text: 5,000,000 characters\n"
     ]
    }
   ],
   "source": [
    "train_text = dataset[\"train\"][0][\"text\"]\n",
    "val_text = dataset[\"validation\"][0][\"text\"]\n",
    "\n",
    "print(f\"Length of train text: {len(train_text):,} characters\")\n",
    "print(f\"Length of validation text: {len(val_text):,} characters\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7b7765a",
   "metadata": {},
   "source": [
    "### Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a388e420",
   "metadata": {},
   "source": [
    "#### Build Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ffe463a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test string 1: hello world\n"
     ]
    }
   ],
   "source": [
    "# Build vocabulary (lowercase + space + a few punctuations)\n",
    "char_set = list(\"abcdefghijklmnopqrstuvwxyz \")\n",
    "char_to_int = {ch:i for i,ch in enumerate(char_set)}\n",
    "int_to_char = {i:ch for ch,i in char_to_int.items()}\n",
    "\n",
    "def encode(s):\n",
    "    \"\"\"Encode string to array of integers\"\"\"\n",
    "    ids = [char_to_int[c] for c in s]\n",
    "    return np.array(ids, dtype=np.uint8)  # use np.uint8 to save space\n",
    "\n",
    "def decode(ids):\n",
    "    \"\"\"Decode array of integers to string\"\"\"\n",
    "    return ''.join(int_to_char[i] for i in ids)\n",
    "# Test encoding and decoding\n",
    "test_str = \"hello world\"\n",
    "encoded = encode(test_str)\n",
    "decoded = decode(encoded)\n",
    "assert test_str == decoded, \"Encoding/decoding failed\"\n",
    "print(f\"Test string 1: {test_str}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e94185ba",
   "metadata": {},
   "source": [
    "#### Tokenize the Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bc9bba12",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_text_int = encode(train_text)\n",
    "test_text_int = encode(val_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "21c1e3f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "some cases the entire main roof of a tower or spire can form a single cupola more frequently however the cupola comprises a smaller structure which sits on top of the main roof if the cupola can be reached by climbing a stairway inside the building it is r\n",
      "\n",
      " to be between four to five years but because new treatments continue to be developed and because hiv continues to evolve resistance to treatments estimates of survival time are likely to continue to change without antiretroviral therapy progression to dea\n",
      "\n",
      " judah history of the middle east the following is a comprehensive list of all persian empires and their rulers early realms in iran elamite kingdom three zero zero zero six six zero bc the elamites were a people located in susa in what is now khuzestan pr\n",
      "\n",
      "ight births one nine six four deaths finnish writers finnish nobel prize winners nobel prize in literature winners february two seven is the five eight th day of the year in the gregorian calendar there are three zero seven days remaining three zero eight \n",
      "\n",
      "merica the term latino refers loosely to any person of latin american origin living in the u s it is typically contrasted with anglo american and or african american in common speech more widely it is occasionally used to denote anyone who speaks or whose \n",
      "\n",
      "es firing masses of rockets at the b one seven formations while this tactic was successful there were too many b one seven s and too few me two six two s to make a real difference the actual number of b one seven s lost to me two six two s using this tacti\n",
      "\n",
      "glass gallium metal expands by three one percent when it solidifies and therefore should not be stored in either glass or metal containers as they may break as the metal solidifies gallium also corrodes most other metals by diffusing into their metal latti\n",
      "\n",
      "a two seater powered by a bristol jupiter viii armed with one vickers machine gun and one lewis gun carrying either one torpedo or a maximum of one zero zero zero lb five zero zero kg of bombs georges remi may two three one nine zero seven march three one \n",
      "\n",
      "m of its levels while many past first person shooter games like doom took place within relatively confined corridors usually in gloomy claustrophobic bases the levels of duke nukem three d took the player through attractively rendered street scenes militar\n",
      "\n",
      " the russian foreign minister sergei lavrov stated that syria should withdraw from lebanon but we all have to make sure that this withdrawal does not violate the very fragile balance which we still have in lebanon which is a very difficult country ethnical\n",
      "\n"
     ]
    }
   ],
   "source": [
    "T = 256\n",
    "\n",
    "for _ in range(10):\n",
    "    N = np.random.randint(0, len(train_text) - T)\n",
    "    print(train_text[N:N+T])\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e1f2cea",
   "metadata": {},
   "source": [
    "### Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7e66dc7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def create_train_state(rng, vocab_size=27, d_model=64, n_layers=6, n_heads=8, max_len=128):\n",
    "#     # create a basic Transformer model\n",
    "#     model = models.DecoderOnlyTransformer(vocab_size, d_model, n_layers, n_heads, max_len)\n",
    "#     # create a dummy input for initialization\n",
    "#     dummy = jnp.zeros((1, min(16, max_len)), dtype=jnp.int32)\n",
    "#     # pass the dummy input to the model to initialize the parameters\n",
    "#     params = model.init({\"params\": rng}, dummy)[\"params\"]\n",
    "#     return model, params\n",
    "\n",
    "\n",
    "def create_train_state(\n",
    "    rng,\n",
    "    vocab_size,\n",
    "    d_model,\n",
    "    n_layers,\n",
    "    n_heads,\n",
    "    max_len,\n",
    "    mlp_ratio,\n",
    "    emb_dropout,\n",
    "    attn_dropout,\n",
    "    mlp_dropout,\n",
    "    param_dtype,\n",
    "    compute_dtype,\n",
    "    kernel_init,\n",
    "    proj_init,\n",
    "):\n",
    "\n",
    "    # Instantiate the model with all given hyperparameters\n",
    "    model = decoder_only_transformer.DecoderOnlyTransformer(\n",
    "        vocab_size=vocab_size,\n",
    "        d_model=d_model,\n",
    "        n_layers=n_layers,\n",
    "        n_heads=n_heads,\n",
    "        max_len=max_len,\n",
    "        mlp_ratio=mlp_ratio,\n",
    "        emb_dropout=emb_dropout,\n",
    "        attn_dropout=attn_dropout,\n",
    "        mlp_dropout=mlp_dropout,\n",
    "        param_dtype=param_dtype,\n",
    "        compute_dtype=compute_dtype,\n",
    "        kernel_init=kernel_init,\n",
    "        proj_init=proj_init,\n",
    "    )\n",
    "\n",
    "    # Create dummy input for initialization (single batch of zeros)\n",
    "    dummy = jnp.zeros((1, min(16, max_len)), dtype=jnp.int32)\n",
    "\n",
    "    # Initialize model parameters (include dropout rng)\n",
    "    variables = model.init({\"params\": rng, \"dropout\": rng}, dummy, deterministic=False)\n",
    "\n",
    "    # Extract parameters only (not batch stats etc.)\n",
    "    params = variables[\"params\"]\n",
    "\n",
    "    return model, params"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36fdb1a1",
   "metadata": {},
   "source": [
    "### Loss Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8e32dbea",
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def loss_and_metrics(logits, targets):\n",
    "    \"\"\"Compute cross-entropy loss and accuracy.\n",
    "\n",
    "    Assumes `targets` contains only valid integer class ids in [0, V-1] (no -1 ignore tokens).\n",
    "\n",
    "    Args:\n",
    "      logits: (B, T, V) float array of unnormalized scores.\n",
    "      targets: (B, T) integer array with ground-truth class ids.\n",
    "\n",
    "    Returns:\n",
    "      loss: scalar average cross-entropy over all positions.\n",
    "      metrics: dict with keys \"loss\" and \"acc\" (both scalars).\n",
    "    \"\"\"\n",
    "    # Flatten batch/time dims so optax works on shape (N, V) and (N,)\n",
    "    vocab = logits.shape[-1]\n",
    "    flat_logits = logits.reshape(-1, vocab)\n",
    "    flat_targets = targets.reshape(-1)\n",
    "\n",
    "    # Per-position cross-entropy, then mean over all positions\n",
    "    per_pos = optax.softmax_cross_entropy_with_integer_labels(flat_logits, flat_targets)\n",
    "    loss = per_pos.mean()\n",
    "\n",
    "    # prediction over all positions\n",
    "    preds = jnp.argmax(logits, axis=-1)  # (B, T)\n",
    "    \n",
    "    # compute accuracy over only the last position\n",
    "    is_match = preds == targets\n",
    "    \n",
    "    # Accuracy over all positions\n",
    "    acc_all = jnp.mean(is_match.astype(jnp.float32))\n",
    "    \n",
    "    # Accuracy over only last position\n",
    "    acc_last = jnp.mean(is_match.astype(jnp.float32)[:,-1])\n",
    "\n",
    "    return loss, {\"loss\": loss, \"acc\": acc_all, \"acc_last\": acc_last}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ace2e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "rng = jax.random.PRNGKey(0)\n",
    "\n",
    "configs = {\n",
    "    \"vocab_size\": len(char_set),\n",
    "    \"d_model\": 256,\n",
    "    \"n_layers\": 6,\n",
    "    \"n_heads\": 8,\n",
    "    \"max_len\": 64,\n",
    "    \"mlp_ratio\": 8,\n",
    "    \"emb_dropout\": 0.1,\n",
    "    \"attn_dropout\": 0.1,\n",
    "    \"mlp_dropout\": 0.1,\n",
    "    \"param_dtype\": jnp.float32,\n",
    "    \"compute_dtype\": jnp.float32,\n",
    "    \"kernel_init\": nn.initializers.xavier_normal(),\n",
    "    \"proj_init\": nn.initializers.normal(stddev=1e-4),\n",
    "    \"learning_rate\": optax.schedules.exponential_decay(\n",
    "        init_value=6e-4,\n",
    "        transition_steps=1000,\n",
    "        decay_rate=0.96,\n",
    "        end_value=1e-5\n",
    "    ),\n",
    "    \"optimizer\": optax.adamw,\n",
    "}\n",
    "\n",
    "model_params = [\n",
    "    \"vocab_size\",\n",
    "    \"d_model\",\n",
    "    \"n_layers\",\n",
    "    \"n_heads\",\n",
    "    \"max_len\",\n",
    "    \"mlp_ratio\",\n",
    "    \"emb_dropout\",\n",
    "    \"attn_dropout\",\n",
    "    \"mlp_dropout\",\n",
    "    \"param_dtype\",\n",
    "    \"compute_dtype\",\n",
    "    \"kernel_init\",\n",
    "    \"proj_init\",\n",
    "]\n",
    "\n",
    "model_configs = {param: configs[param] for param in model_params}\n",
    "\n",
    "model, params = create_train_state(rng, **model_configs)\n",
    "\n",
    "# Run logging with Wandb\n",
    "wandb.login()\n",
    "\n",
    "run = wandb.init(\n",
    "    config=configs,\n",
    "    name=\"Decoder Only Transformer - Test 6\",\n",
    "    notes=\"Batch Size = 128, Context Window = 64, Using Exponential Decay Scheuduler\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "ff754cf2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters: 7,915,008\n"
     ]
    }
   ],
   "source": [
    "def count_params(params):\n",
    "    return sum(x.size for x in jax.tree_util.tree_leaves(params))\n",
    "print(f\"Number of parameters: {count_params(params):,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a711656c",
   "metadata": {},
   "source": [
    "### Optimization Step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d91204bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create an update function\n",
    "def train_step(params, opt_state, x, y, tx):\n",
    "    \"\"\"Single optimization step using optax optimizer.\n",
    "\n",
    "    Args:\n",
    "      params: pytree of model parameters.\n",
    "      opt_state: optax optimizer state corresponding to `params`.\n",
    "      x: (B, T) int array input tokens.\n",
    "      y: (B, T) int array target tokens.\n",
    "      tx: optax.GradientTransformation (already initialized).\n",
    "\n",
    "    Returns:\n",
    "      new_params: updated parameters after one gradient step.\n",
    "      new_opt_state: updated optimizer state.\n",
    "      metrics: dict of scalar metrics (loss, acc).\n",
    "    \"\"\"\n",
    "    def loss_fn(params):\n",
    "        logits = model.apply({\"params\": params}, x)\n",
    "        loss, metrics = loss_and_metrics(logits, y)\n",
    "        return loss, metrics\n",
    "\n",
    "    # compute gradients (loss is scalar, metrics is auxiliary)\n",
    "    (loss, metrics), grads = jax.value_and_grad(loss_fn, has_aux=True)(params)\n",
    "\n",
    "    # optax update: compute parameter updates and new optimizer state\n",
    "    updates, new_opt_state = tx.update(grads, opt_state, params)\n",
    "    new_params = optax.apply_updates(params, updates)\n",
    "    return new_params, new_opt_state, metrics\n",
    "\n",
    "# jit: last argument should be static because it is an object\n",
    "train_step = jax.jit(train_step, static_argnames=(\"tx\",))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bcb38b6",
   "metadata": {},
   "source": [
    "### Batch Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "199ab421",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a batch from the training data\n",
    "def get_batch(text_int, B, T):\n",
    "    \"\"\"Create a random batch of data from text_int.\n",
    "\n",
    "    Args:\n",
    "      text_int: 1D array of token ids.\n",
    "      B: batch size (number of sequences).\n",
    "      T: sequence length (number of tokens per sequence).\n",
    "\n",
    "    Returns:\n",
    "      x: (B, T) int array input tokens.\n",
    "      y: (B, T) int array target tokens.\n",
    "    \"\"\"\n",
    "    # choose random starting indices for each sequence in the batch\n",
    "    ix = np.random.randint(0, len(text_int) - T, size=B)\n",
    "    # inputs are text from i to i+T\n",
    "    x = np.stack([text_int[i:i+T] for i in ix])\n",
    "    # targets are text from i+1 to i+T+1\n",
    "    y = np.stack([text_int[i+1:i+T+1] for i in ix])\n",
    "    return jnp.array(x, dtype=jnp.int32), jnp.array(y, dtype=jnp.int32)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31a16c4a",
   "metadata": {},
   "source": [
    "### Optimizer Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "935c9fe0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized optimizer: AdamW lr=<function exponential_decay.<locals>.schedule at 0x7b9b40338ea0>\n"
     ]
    }
   ],
   "source": [
    "# define optax optimizer\n",
    "learning_rate = configs[\"learning_rate\"]\n",
    "# Create Adam optimizer (Optax)\n",
    "tx = configs[\"optimizer\"](learning_rate=learning_rate)\n",
    "# Initialize optimizer state for current params\n",
    "opt_state = tx.init(params)\n",
    "print(f\"Initialized optimizer: AdamW lr={learning_rate}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1422e1ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 1  time: 19.8 seconds\n",
      "\t \t loss(train :: test): 3.2961 :: 3.2824\n",
      "\t \t accuracy (train :: test): 2.1% :: 14.0%\n",
      "\t \t accuracy (last character) (train :: test): 1.2% :: 13.8%\n",
      "\n",
      "iteration 2_000  time: 457.3 seconds\n",
      "\t \t loss(train :: test): 1.3124 :: 1.3420\n",
      "\t \t accuracy (train :: test): 58.7% :: 57.8%\n",
      "\t \t accuracy (last character) (train :: test): 59.4% :: 60.9%\n",
      "\n",
      "iteration 4_000  time: 893.8 seconds\n",
      "\t \t loss(train :: test): 1.2375 :: 1.2716\n",
      "\t \t accuracy (train :: test): 60.8% :: 59.7%\n",
      "\t \t accuracy (last character) (train :: test): 60.5% :: 62.5%\n",
      "\n",
      "iteration 6_000  time: 1106.1 seconds\n",
      "\t \t loss(train :: test): 1.2053 :: 1.2472\n",
      "\t \t accuracy (train :: test): 62.0% :: 60.4%\n",
      "\t \t accuracy (last character) (train :: test): 65.6% :: 61.0%\n",
      "\n",
      "iteration 8_000  time: 1322.4 seconds\n",
      "\t \t loss(train :: test): 1.2034 :: 1.2372\n",
      "\t \t accuracy (train :: test): 62.6% :: 61.0%\n",
      "\t \t accuracy (last character) (train :: test): 65.2% :: 66.6%\n",
      "\n",
      "iteration 10_000  time: 1541.0 seconds\n",
      "\t \t loss(train :: test): 1.1453 :: 1.2252\n",
      "\t \t accuracy (train :: test): 63.7% :: 61.2%\n",
      "\t \t accuracy (last character) (train :: test): 68.0% :: 67.5%\n",
      "\n",
      "iteration 12_000  time: 1760.9 seconds\n",
      "\t \t loss(train :: test): 1.1522 :: 1.2071\n",
      "\t \t accuracy (train :: test): 63.5% :: 62.0%\n",
      "\t \t accuracy (last character) (train :: test): 68.4% :: 67.7%\n",
      "\n",
      "iteration 14_000  time: 1981.8 seconds\n",
      "\t \t loss(train :: test): 1.1527 :: 1.1977\n",
      "\t \t accuracy (train :: test): 63.7% :: 62.2%\n",
      "\t \t accuracy (last character) (train :: test): 62.9% :: 66.4%\n",
      "\n",
      "iteration 16_000  time: 2202.1 seconds\n",
      "\t \t loss(train :: test): 1.1217 :: 1.1993\n",
      "\t \t accuracy (train :: test): 64.4% :: 62.3%\n",
      "\t \t accuracy (last character) (train :: test): 61.3% :: 66.1%\n",
      "\n",
      "iteration 18_000  time: 2422.8 seconds\n",
      "\t \t loss(train :: test): 1.1238 :: 1.1928\n",
      "\t \t accuracy (train :: test): 64.3% :: 62.1%\n",
      "\t \t accuracy (last character) (train :: test): 60.2% :: 66.6%\n",
      "\n",
      "iteration 20_000  time: 2642.9 seconds\n",
      "\t \t loss(train :: test): 1.1123 :: 1.1772\n",
      "\t \t accuracy (train :: test): 64.8% :: 62.8%\n",
      "\t \t accuracy (last character) (train :: test): 65.6% :: 67.8%\n",
      "\n",
      "iteration 22_000  time: 2861.3 seconds\n",
      "\t \t loss(train :: test): 1.0926 :: 1.1790\n",
      "\t \t accuracy (train :: test): 65.3% :: 62.9%\n",
      "\t \t accuracy (last character) (train :: test): 73.8% :: 64.6%\n",
      "\n",
      "iteration 24_000  time: 3080.4 seconds\n",
      "\t \t loss(train :: test): 1.0632 :: 1.1498\n",
      "\t \t accuracy (train :: test): 66.2% :: 63.5%\n",
      "\t \t accuracy (last character) (train :: test): 68.0% :: 68.4%\n",
      "\n",
      "iteration 26_000  time: 3300.9 seconds\n",
      "\t \t loss(train :: test): 1.1088 :: 1.1695\n",
      "\t \t accuracy (train :: test): 65.0% :: 63.2%\n",
      "\t \t accuracy (last character) (train :: test): 67.6% :: 66.4%\n",
      "\n",
      "iteration 28_000  time: 3519.2 seconds\n",
      "\t \t loss(train :: test): 1.0702 :: 1.1827\n",
      "\t \t accuracy (train :: test): 65.9% :: 62.6%\n",
      "\t \t accuracy (last character) (train :: test): 66.4% :: 65.6%\n",
      "\n",
      "iteration 30_000  time: 3737.7 seconds\n",
      "\t \t loss(train :: test): 1.0993 :: 1.1650\n",
      "\t \t accuracy (train :: test): 65.5% :: 63.3%\n",
      "\t \t accuracy (last character) (train :: test): 71.1% :: 69.8%\n",
      "\n",
      "iteration 32_000  time: 3957.9 seconds\n",
      "\t \t loss(train :: test): 1.0696 :: 1.1516\n",
      "\t \t accuracy (train :: test): 66.2% :: 63.6%\n",
      "\t \t accuracy (last character) (train :: test): 69.1% :: 69.8%\n",
      "\n",
      "iteration 34_000  time: 4326.0 seconds\n",
      "\t \t loss(train :: test): 1.0587 :: 1.1577\n",
      "\t \t accuracy (train :: test): 66.6% :: 63.6%\n",
      "\t \t accuracy (last character) (train :: test): 68.4% :: 67.2%\n",
      "\n",
      "iteration 36_000  time: 4772.3 seconds\n",
      "\t \t loss(train :: test): 1.0892 :: 1.1330\n",
      "\t \t accuracy (train :: test): 65.4% :: 64.1%\n",
      "\t \t accuracy (last character) (train :: test): 68.8% :: 69.5%\n",
      "\n",
      "iteration 38_000  time: 5211.5 seconds\n",
      "\t \t loss(train :: test): 1.0566 :: 1.1333\n",
      "\t \t accuracy (train :: test): 66.9% :: 64.5%\n",
      "\t \t accuracy (last character) (train :: test): 66.4% :: 69.7%\n",
      "\n",
      "iteration 40_000  time: 5646.8 seconds\n",
      "\t \t loss(train :: test): 1.0406 :: 1.1587\n",
      "\t \t accuracy (train :: test): 66.9% :: 63.7%\n",
      "\t \t accuracy (last character) (train :: test): 62.5% :: 64.7%\n",
      "\n",
      "iteration 42_000  time: 5980.5 seconds\n",
      "\t \t loss(train :: test): 1.0748 :: 1.1495\n",
      "\t \t accuracy (train :: test): 66.1% :: 63.6%\n",
      "\t \t accuracy (last character) (train :: test): 62.5% :: 71.9%\n",
      "\n",
      "iteration 44_000  time: 6199.4 seconds\n",
      "\t \t loss(train :: test): 1.0458 :: 1.1450\n",
      "\t \t accuracy (train :: test): 67.0% :: 64.5%\n",
      "\t \t accuracy (last character) (train :: test): 66.0% :: 71.3%\n",
      "\n",
      "iteration 46_000  time: 6421.4 seconds\n",
      "\t \t loss(train :: test): 1.0435 :: 1.1365\n",
      "\t \t accuracy (train :: test): 67.3% :: 64.4%\n",
      "\t \t accuracy (last character) (train :: test): 72.3% :: 72.2%\n",
      "\n",
      "iteration 48_000  time: 6647.8 seconds\n",
      "\t \t loss(train :: test): 1.0709 :: 1.1448\n",
      "\t \t accuracy (train :: test): 66.4% :: 64.2%\n",
      "\t \t accuracy (last character) (train :: test): 71.1% :: 68.8%\n",
      "\n",
      "iteration 50_000  time: 6875.7 seconds\n",
      "\t \t loss(train :: test): 1.0377 :: 1.1383\n",
      "\t \t accuracy (train :: test): 67.3% :: 63.6%\n",
      "\t \t accuracy (last character) (train :: test): 70.3% :: 68.5%\n",
      "\n",
      "iteration 52_000  time: 7104.9 seconds\n",
      "\t \t loss(train :: test): 1.0405 :: 1.1412\n",
      "\t \t accuracy (train :: test): 67.1% :: 64.1%\n",
      "\t \t accuracy (last character) (train :: test): 67.6% :: 69.8%\n",
      "\n",
      "iteration 54_000  time: 7331.8 seconds\n",
      "\t \t loss(train :: test): 1.0658 :: 1.1498\n",
      "\t \t accuracy (train :: test): 66.3% :: 64.2%\n",
      "\t \t accuracy (last character) (train :: test): 69.1% :: 68.7%\n",
      "\n",
      "iteration 56_000  time: 7556.0 seconds\n",
      "\t \t loss(train :: test): 1.0472 :: 1.1469\n",
      "\t \t accuracy (train :: test): 67.1% :: 63.9%\n",
      "\t \t accuracy (last character) (train :: test): 70.3% :: 67.0%\n",
      "\n",
      "iteration 58_000  time: 7780.5 seconds\n",
      "\t \t loss(train :: test): 1.0357 :: 1.1571\n",
      "\t \t accuracy (train :: test): 66.6% :: 63.7%\n",
      "\t \t accuracy (last character) (train :: test): 66.0% :: 67.8%\n",
      "\n",
      "iteration 60_000  time: 8008.2 seconds\n",
      "\t \t loss(train :: test): 1.0416 :: 1.1519\n",
      "\t \t accuracy (train :: test): 67.0% :: 63.6%\n",
      "\t \t accuracy (last character) (train :: test): 66.8% :: 69.0%\n",
      "\n",
      "iteration 62_000  time: 8237.8 seconds\n",
      "\t \t loss(train :: test): 1.0670 :: 1.1550\n",
      "\t \t accuracy (train :: test): 66.2% :: 63.8%\n",
      "\t \t accuracy (last character) (train :: test): 71.1% :: 68.4%\n",
      "\n",
      "iteration 64_000  time: 8463.3 seconds\n",
      "\t \t loss(train :: test): 1.0449 :: 1.1451\n",
      "\t \t accuracy (train :: test): 66.9% :: 63.9%\n",
      "\t \t accuracy (last character) (train :: test): 72.7% :: 69.9%\n",
      "\n",
      "iteration 66_000  time: 8688.3 seconds\n",
      "\t \t loss(train :: test): 1.0333 :: 1.1497\n",
      "\t \t accuracy (train :: test): 67.5% :: 64.1%\n",
      "\t \t accuracy (last character) (train :: test): 71.5% :: 70.4%\n",
      "\n",
      "iteration 68_000  time: 8912.1 seconds\n",
      "\t \t loss(train :: test): 1.0602 :: 1.1343\n",
      "\t \t accuracy (train :: test): 66.7% :: 64.2%\n",
      "\t \t accuracy (last character) (train :: test): 66.8% :: 69.1%\n",
      "\n",
      "iteration 70_000  time: 9197.9 seconds\n",
      "\t \t loss(train :: test): 1.0059 :: 1.1404\n",
      "\t \t accuracy (train :: test): 68.2% :: 64.2%\n",
      "\t \t accuracy (last character) (train :: test): 74.2% :: 70.7%\n",
      "\n",
      "iteration 72_000  time: 9659.5 seconds\n",
      "\t \t loss(train :: test): 1.0335 :: 1.1335\n",
      "\t \t accuracy (train :: test): 67.5% :: 64.3%\n",
      "\t \t accuracy (last character) (train :: test): 69.1% :: 71.6%\n",
      "\n",
      "iteration 74_000  time: 10092.6 seconds\n",
      "\t \t loss(train :: test): 1.0163 :: 1.1427\n",
      "\t \t accuracy (train :: test): 67.9% :: 64.1%\n",
      "\t \t accuracy (last character) (train :: test): 74.2% :: 68.8%\n",
      "\n",
      "iteration 76_000  time: 10532.7 seconds\n",
      "\t \t loss(train :: test): 1.0445 :: 1.1202\n",
      "\t \t accuracy (train :: test): 67.0% :: 64.8%\n",
      "\t \t accuracy (last character) (train :: test): 69.1% :: 71.4%\n",
      "\n",
      "iteration 78_000  time: 10886.6 seconds\n",
      "\t \t loss(train :: test): 1.0376 :: 1.1341\n",
      "\t \t accuracy (train :: test): 67.1% :: 64.4%\n",
      "\t \t accuracy (last character) (train :: test): 69.9% :: 70.7%\n",
      "\n",
      "iteration 80_000  time: 11103.6 seconds\n",
      "\t \t loss(train :: test): 1.0323 :: 1.1303\n",
      "\t \t accuracy (train :: test): 67.4% :: 64.7%\n",
      "\t \t accuracy (last character) (train :: test): 70.3% :: 68.1%\n",
      "\n",
      "iteration 82_000  time: 11325.9 seconds\n",
      "\t \t loss(train :: test): 1.0176 :: 1.1442\n",
      "\t \t accuracy (train :: test): 68.0% :: 64.1%\n",
      "\t \t accuracy (last character) (train :: test): 73.0% :: 69.1%\n",
      "\n",
      "iteration 84_000  time: 11551.3 seconds\n",
      "\t \t loss(train :: test): 1.0151 :: 1.1280\n",
      "\t \t accuracy (train :: test): 68.3% :: 64.6%\n",
      "\t \t accuracy (last character) (train :: test): 70.3% :: 68.5%\n",
      "\n",
      "iteration 86_000  time: 11779.1 seconds\n",
      "\t \t loss(train :: test): 0.9944 :: 1.1405\n",
      "\t \t accuracy (train :: test): 68.1% :: 64.2%\n",
      "\t \t accuracy (last character) (train :: test): 70.7% :: 67.1%\n",
      "\n",
      "iteration 88_000  time: 12006.9 seconds\n",
      "\t \t loss(train :: test): 1.0239 :: 1.1392\n",
      "\t \t accuracy (train :: test): 67.9% :: 64.4%\n",
      "\t \t accuracy (last character) (train :: test): 73.0% :: 68.1%\n",
      "\n",
      "iteration 90_000  time: 12238.0 seconds\n",
      "\t \t loss(train :: test): 1.0079 :: 1.1364\n",
      "\t \t accuracy (train :: test): 68.0% :: 64.1%\n",
      "\t \t accuracy (last character) (train :: test): 75.4% :: 70.3%\n",
      "\n",
      "iteration 92_000  time: 12464.6 seconds\n",
      "\t \t loss(train :: test): 1.0252 :: 1.1304\n",
      "\t \t accuracy (train :: test): 67.4% :: 64.4%\n",
      "\t \t accuracy (last character) (train :: test): 70.3% :: 68.7%\n",
      "\n",
      "iteration 94_000  time: 12691.9 seconds\n",
      "\t \t loss(train :: test): 1.0549 :: 1.1440\n",
      "\t \t accuracy (train :: test): 67.2% :: 64.1%\n",
      "\t \t accuracy (last character) (train :: test): 73.4% :: 67.5%\n",
      "\n",
      "iteration 96_000  time: 12919.1 seconds\n",
      "\t \t loss(train :: test): 1.0331 :: 1.1512\n",
      "\t \t accuracy (train :: test): 67.7% :: 64.2%\n",
      "\t \t accuracy (last character) (train :: test): 69.5% :: 70.1%\n",
      "\n",
      "iteration 98_000  time: 13144.5 seconds\n",
      "\t \t loss(train :: test): 1.0314 :: 1.1352\n",
      "\t \t accuracy (train :: test): 67.8% :: 64.3%\n",
      "\t \t accuracy (last character) (train :: test): 75.0% :: 70.0%\n",
      "\n",
      "iteration 100_000  time: 13371.3 seconds\n",
      "\t \t loss(train :: test): 1.0257 :: 1.1403\n",
      "\t \t accuracy (train :: test): 67.7% :: 64.0%\n",
      "\t \t accuracy (last character) (train :: test): 69.9% :: 71.2%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "niter = 100_000\n",
    "B, T = 256, 64\n",
    "loss_history = []\n",
    "time_history = []\n",
    "time_test_history = []\n",
    "loss_test_history = []\n",
    "time_start = time.time()\n",
    "for it in range(1, niter + 1):\n",
    "    batch = get_batch(train_text_int, B, T)\n",
    "    input, target = batch[0], batch[1]\n",
    "    params_new, opt_state_new, metrics = train_step(params, opt_state, input, target, tx)\n",
    "\n",
    "    # update params and opt_state\n",
    "    params = params_new\n",
    "    opt_state = opt_state_new\n",
    "    acc = metrics['acc']\n",
    "    acc_last = metrics['acc_last']\n",
    "    loss = metrics['loss']\n",
    "\n",
    "    loss_history.append(loss)\n",
    "    time_history.append(time.time() - time_start)\n",
    "    run.log({\n",
    "        \"loss/train\": loss, \n",
    "        \"train_accuracy\": acc, \n",
    "        \"train_last_character_accuracy\": acc_last,\n",
    "    })\n",
    "\n",
    "    if it % (niter // 50) == 0 or it == 1:\n",
    "        time_since_start = time.time() - time_start\n",
    "        # compute loss on test set\n",
    "        B_test, T_test = 1024, 32\n",
    "        test_batch = get_batch(test_text_int, B_test, T_test)\n",
    "        test_input, test_target = test_batch[0], test_batch[1]\n",
    "        test_logits = model.apply({\"params\": params}, test_input)\n",
    "        test_loss, test_metrics = loss_and_metrics(test_logits, test_target)\n",
    "        test_acc = test_metrics['acc']\n",
    "        test_acc_last = test_metrics['acc_last']\n",
    "        loss_test_history.append(test_loss)\n",
    "        time_test_history.append(time_since_start)\n",
    "        run.log({\n",
    "            \"loss/test\": test_loss, \n",
    "            \"test_accuracy\": test_acc, \n",
    "            \"test_last_character_accuracy\": test_acc_last,\n",
    "        })\n",
    "        print(f\"iteration {it:_}  time: {time_since_start:.1f} seconds\")\n",
    "        print(f\"\\t \\t loss(train :: test): {loss:.4f} :: {test_loss:.4f}\")\n",
    "        print(f\"\\t \\t accuracy (train :: test): {100*acc:.1f}% :: {100*test_acc:.1f}%\")\n",
    "        print(f\"\\t \\t accuracy (last character) (train :: test): {100*acc_last:.1f}% :: {100*test_acc_last:.1f}%\")\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "0a9fe7da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(time_history, loss_history, '-', label='train', color=\"blue\")\n",
    "plt.plot(time_test_history, loss_test_history, '-', label='test', lw=2, color=\"red\")\n",
    "plt.xlabel(\"Time (seconds)\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend(loc='upper right')\n",
    "plt.title(\"Training Loss History\")\n",
    "plt.grid()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc8a2074",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "205906e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_categorical(rng, logits, temperature=1.0):\n",
    "    \"\"\"Sample from categorical distribution given by logits with temperature.\"\"\"\n",
    "    # logits: (B, V)\n",
    "    if temperature != 1.0:\n",
    "        logits = logits / temperature\n",
    "    return jax.random.categorical(rng, logits, axis=-1)\n",
    "\n",
    "#@functools.partial(jax.jit, static_argnames=(\"model\", \"block_size\", \"temperature\", \"sample\"))\n",
    "def generate_tokens(model, params, rng, context, length, block_size=64, temperature=1.0, sample=True):\n",
    "    \"\"\"Generate `length` new tokens autoregressively starting from `context`.\n",
    "\n",
    "    Args:\n",
    "      model: Flax/linen model object with apply signature model.apply({'params': params}, tokens)\n",
    "      params: model parameters pytree.\n",
    "      rng: jax PRNGKey (e.g., jax.random.PRNGKey(0)).\n",
    "      context: int32 array shape (B, S) where S <= block_size. If S < block_size, left-pad with zeros or use full context.\n",
    "      length: number of tokens to generate (int).\n",
    "      block_size: model context window (static).\n",
    "      temperature: sampling temperature (static).\n",
    "      sample: if True use sampling; if False use argmax (greedy).\n",
    "\n",
    "    Returns:\n",
    "      generated: int32 array shape (B, length) of generated token ids.\n",
    "    \"\"\"\n",
    "    B = context.shape[0]\n",
    "    S = context.shape[1]\n",
    "    assert S <= block_size, \"context length must be <= block_size\"\n",
    "\n",
    "    # initialize running context: if S < block_size, left-pad with zeros (or use a preferred pad token)\n",
    "    if S < block_size:\n",
    "        pad_len = block_size - S\n",
    "        context = jnp.concatenate([jnp.zeros((B, pad_len), dtype=jnp.int32), context], axis=1)\n",
    "\n",
    "    def _step(carry, _):\n",
    "        rng, ctx = carry  # rng: PRNGKey, ctx: (B, block_size)\n",
    "        # forward pass: get logits for all positions, take last position\n",
    "        logits = model.apply({\"params\": params}, ctx)  # (B, block_size, V)\n",
    "        last_logits = logits[:, -1, :]  # (B, V)\n",
    "        rng, subkey = jax.random.split(rng)\n",
    "        if sample:\n",
    "            next_token = jax.random.categorical(subkey, last_logits / (temperature if temperature>0 else 1.0), axis=-1)\n",
    "        else:\n",
    "            next_token = jnp.argmax(last_logits, axis=-1)\n",
    "        next_token = next_token.astype(jnp.int32)\n",
    "        # append to context: drop first token, append new token at end\n",
    "        next_token_col = next_token.reshape(B, 1)\n",
    "        new_ctx = jnp.concatenate([ctx[:, 1:], next_token_col], axis=1)\n",
    "        return (rng, new_ctx), next_token_col\n",
    "\n",
    "    # run scan for `length` steps\n",
    "    (rng_final, ctx_final), tokens = jax.lax.scan(_step, (rng, context), None, length=length)\n",
    "    # tokens: shape (length, B, 1) -> reshape to (B, length)\n",
    "    tokens = tokens.squeeze(-1).transpose(1, 0)\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "fb95fe1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generated ids shape: (1, 1000)\n",
      "generated text:\n",
      "hello my friend and woodrow wilson in a man in the same fashion and contributed to the multi user content capability the multiple releases of any game the board are a projection of such an account of the transformation which is responsible for the complete electromagnetic force the composite of the local government of hong kong external links english language school of english language computer science language or the english language a manual of x is considered a comprehensive term used to describe a feature of hindu manifestations of the poem grace and that it could be less likely to have described his lectures see also concurrent effects of great amounts of learning and compatibility for the file system information in the process that are operated by allowing the speakers to develop and participate in the contempt of the problem in the product the more detailed information to a design and investigation into the us and came to many of these were made by the united states some believe that all ar\n"
     ]
    }
   ],
   "source": [
    "B = 1\n",
    "seed = 42\n",
    "rng = jax.random.PRNGKey(seed)\n",
    "prompt = \"hello my fri\"\n",
    "# prompt_int = encode(prompt.lower())\n",
    "prompt_int = jnp.array([ [char_to_int.get(c, len(char_set)) for c in prompt.lower()[:64]] ], dtype=jnp.int32)\n",
    "\n",
    "gen_len = 1000\n",
    "out_ids = generate_tokens(model, params, rng, prompt_int, gen_len, block_size=64, \n",
    "                          temperature=0.7, sample=True)\n",
    "print('generated ids shape:', out_ids.shape)\n",
    "print('generated text:')\n",
    "generated_text = ''.join(int_to_char.get(int(x), '?') for x in list(out_ids[0]))\n",
    "# concatenate with prompt\n",
    "print(prompt + generated_text)\n",
    "#print(''.join(int_to_char.get(int(x), '?') for x in list(out_ids[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "844d8374",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('decoder_only_transformer_params-test-6.pkl', 'wb') as f:\n",
    "    pickle.dump(params, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8c640ea7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Final Test Evaluation (80-20 split) ---\n",
      "Average Test Loss: 1.1090\n",
      "Accuracy (all positions): 65.35%\n",
      "Accuracy (next-character / last position): 68.31%\n"
     ]
    }
   ],
   "source": [
    "with open('decoder_only_transformer_params-test-6.pkl', 'rb') as f:\n",
    "    loaded_params = pickle.load(f)\n",
    "\n",
    "test_text = dataset[\"test\"][0][\"text\"]\n",
    "test_text_enc = encode(test_text)\n",
    "\n",
    "B_test = 128\n",
    "T_test = 64\n",
    "num_test_batches = 50\n",
    "\n",
    "acc_hist = []\n",
    "acc_last_hist = []\n",
    "test_loss_hist = []\n",
    "\n",
    "for _ in range(num_test_batches):\n",
    "    x_test, y_test = get_batch(test_text_enc, B_test, T_test)\n",
    "\n",
    "    logits_test = model.apply({\"params\": loaded_params}, x_test, deterministic=True)\n",
    "\n",
    "    test_loss, test_metrics = loss_and_metrics(logits_test, y_test)\n",
    "\n",
    "    test_loss_hist.append(float(test_loss))\n",
    "    acc_hist.append(float(test_metrics[\"acc\"]))\n",
    "    acc_last_hist.append(float(test_metrics[\"acc_last\"]))\n",
    "\n",
    "avg_test_loss = np.mean(test_loss_hist)\n",
    "avg_acc_all = np.mean(acc_hist) * 100\n",
    "avg_acc_last = np.mean(acc_last_hist) * 100\n",
    "\n",
    "print(\"\\n--- Final Test Evaluation (80-20 split) ---\")\n",
    "print(f\"Average Test Loss: {avg_test_loss:.4f}\")\n",
    "print(f\"Accuracy (all positions): {avg_acc_all:.2f}%\")\n",
    "print(f\"Accuracy (next-character / last position): {avg_acc_last:.2f}%\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce132cd7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
