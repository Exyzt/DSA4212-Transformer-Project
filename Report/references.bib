@misc{vaswani_2017_attention,
  author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N. and Kaiser, Lukasz and Polosukhin, Illia},
  title = {Attention Is All You Need},
  doi = {10.48550/arXiv.1706.03762},
  url = {https://arxiv.org/abs/1706.03762},
  year = {2017},
  organization = {arXiv.org}
}

@misc{olson_2023_exgoogle,
  author = {Olson, Parmy},
  month = {07},
  publisher = {Bloomberg},
  title = {Ex-Google Scientists Kickstarted the Generative AI Era of ChatGPT, Midjourney},
  url = {https://www.bloomberg.com/opinion/features/2023-07-13/ex-google-scientists-kickstarted-the-generative-ai-era-of-chatgpt-midjourney?embedded-checkout=true},
  urldate = {2025-11-16},
  year = {2023},
  organization = {Bloomberg.com}
}

@misc{phd_2024_decoderonly,
  author = {Ph.D, Cameron R. Wolfe},
  month = {03},
  title = {Decoder-Only Transformers: The Workhorse of Generative LLMs},
  url = {https://cameronrwolfe.substack.com/p/decoder-only-transformers-the-workhorse},
  year = {2024},
  organization = {Deep (Learning) Focus}
}

@misc{muhammadardi_2025_meet,
  author = {Muhammad Ardi},
  month = {01},
  title = {Meet GPT, The Decoder-Only Transformer | Towards Data Science},
  url = {https://towardsdatascience.com/meet-gpt-the-decoder-only-transformer-12f4a7918b36/},
  year = {2025},
  organization = {Towards Data Science}
}

@misc{saeed_2022_a,
  author = {Saeed, Mehreen},
  month = {01},
  title = {A Gentle Introduction to Positional Encoding In Transformer Models, Part 1},
  url = {https://machinelearningmastery.com/a-gentle-introduction-to-positional-encoding-in-transformer-models-part-1/},
  year = {2022},
  organization = {Machine Learning Mastery}
}

@misc{tam_2025_positional,
  author = {Tam, Adrian},
  month = {06},
  title = {Positional Encodings in Transformer Models - MachineLearningMastery.com},
  url = {https://machinelearningmastery.com/positional-encodings-in-transformer-models/},
  year = {2025},
  organization = {MachineLearningMastery.com}
}

@misc{wang_2022_a,
  author = {Wang, Guoxin and Lu, Yijuan and Cui, Lei and Lv, Tengchao and Florencio, Dinei and Zhang, Cha},
  pages = {453-463},
  title = {A Simple yet Effective Learnable Positional Encoding Method for Improving Document Transformer Model},
  url = {https://aclanthology.org/2022.findings-aacl.42.pdf},
  year = {2022}
}

@misc{prashunjaveri_2024_gpt,
  author = {prashun javeri},
  month = {06},
  title = {GPT Architecture},
  url = {https://medium.com/@prashunjaveri/gpt-architecture-0415e7a5796d},
  urldate = {2025-11-20},
  year = {2024},
  organization = {Medium}
}

@misc{su_2021_roformer,
  author = {Su, Jianlin and Lu, Yu and Pan, Sheng-Feng and Wen, Bo and Liu, Yunfeng},
  month = {04},
  title = {RoFormer: Enhanced Transformer with Rotary Position Embedding},
  doi = {10.48550/arxiv.2104.09864},
  year = {2021},
}

@misc{biderman_2021_rotary,
  author = {Biderman, Stella and Black, Sid and Foster, Charles and Gao, Leo and Hallahan, Eric and He, Horace and Wang, Ben and Wang, Phil},
  month = {04},
  title = {Rotary Embeddings: A Relative Revolution},
  url = {https://blog.eleuther.ai/rotary-embeddings/},
  year = {2021},
  organization = {EleutherAI Blog}
}

@misc{a2025_rotary,
  title = {Rotary Positional Embeddings (RoPE)},
  url = {https://nn.labml.ai/transformers/rope/index.html},
  urldate = {2025-11-20},
  year = {2025},
}

@misc{shubham_2025_inside,
  author = {Shubham},
  month = {07},
  publisher = {LearnOpenCV},
  title = {Inside RoPE: Rotary Magic into Position Embeddings},
  url = {https://learnopencv.com/rope-position-embeddings/},
  urldate = {2025-11-20},
  year = {2025},
  organization = {LearnOpenCV â€“ Learn OpenCV, PyTorch, Keras, Tensorflow with code, & tutorials}
}